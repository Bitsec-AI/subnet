"""
Vulnerability benchmarking module for the Bitsec subnet.

This module provides tools to evaluate and compare the performance of miners
on different types of security vulnerabilities.
"""

import logging
import json
import os
from pathlib import Path
from typing import Dict, List, Set, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from datetime import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from io import BytesIO
import base64
import traceback
from collections import defaultdict

@dataclass
class BenchmarkResult:
    """
    Data class for storing benchmark results for a specific vulnerability type.
    
    Attributes:
        vulnerability_type: Type of vulnerability being benchmarked
        miner_id: ID of the miner being evaluated
        true_positives: Number of true positives detected
        false_positives: Number of false positives detected
        false_negatives: Number of false negatives (missed vulnerabilities)
        precision: Precision score (true positives / (true positives + false positives))
        recall: Recall score (true positives / (true positives + false negatives))
        f1_score: F1 score (harmonic mean of precision and recall)
        avg_response_time: Average response time for this vulnerability type
        timestamp: When the benchmark was run
    """
    vulnerability_type: str
    miner_id: str
    true_positives: int
    false_positives: int
    false_negatives: int
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    avg_response_time: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)
    
    def __post_init__(self):
        """Calculate precision, recall, and F1 score if not explicitly set."""
        # Calculate precision
        if self.true_positives + self.false_positives > 0:
            self.precision = self.true_positives / (self.true_positives + self.false_positives)
        
        # Calculate recall
        if self.true_positives + self.false_negatives > 0:
            self.recall = self.true_positives / (self.true_positives + self.false_negatives)
        
        # Calculate F1 score
        if self.precision + self.recall > 0:
            self.f1_score = 2 * (self.precision * self.recall) / (self.precision + self.recall)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "vulnerability_type": self.vulnerability_type,
            "miner_id": self.miner_id,
            "true_positives": self.true_positives,
            "false_positives": self.false_positives,
            "false_negatives": self.false_negatives,
            "precision": self.precision,
            "recall": self.recall,
            "f1_score": self.f1_score,
            "avg_response_time": self.avg_response_time,
            "timestamp": self.timestamp.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'BenchmarkResult':
        """Create from dictionary after deserialization."""
        return cls(
            vulnerability_type=data.get("vulnerability_type", ""),
            miner_id=data.get("miner_id", ""),
            true_positives=data.get("true_positives", 0),
            false_positives=data.get("false_positives", 0),
            false_negatives=data.get("false_negatives", 0),
            precision=data.get("precision", 0.0),
            recall=data.get("recall", 0.0),
            f1_score=data.get("f1_score", 0.0),
            avg_response_time=data.get("avg_response_time", 0.0),
            timestamp=datetime.fromisoformat(data.get("timestamp", datetime.now().isoformat()))
        )

@dataclass
class VulnerabilityTestCase:
    """
    Data class representing a test case for a specific vulnerability type.
    
    Attributes:
        vulnerability_type: Type of vulnerability being tested
        name: Name of the test case
        code: Code sample containing the vulnerability
        labeled_vulnerabilities: Ground-truth vulnerabilities for evaluation
        description: Description of the test case
        expected_severity: Expected severity level for this vulnerability
    """
    vulnerability_type: str
    name: str
    code: str
    labeled_vulnerabilities: List[Dict[str, Any]]
    description: str = ""
    expected_severity: float = 5.0
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "vulnerability_type": self.vulnerability_type,
            "name": self.name,
            "code": self.code,
            "labeled_vulnerabilities": self.labeled_vulnerabilities,
            "description": self.description,
            "expected_severity": self.expected_severity
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'VulnerabilityTestCase':
        """Create from dictionary after deserialization."""
        return cls(
            vulnerability_type=data.get("vulnerability_type", ""),
            name=data.get("name", ""),
            code=data.get("code", ""),
            labeled_vulnerabilities=data.get("labeled_vulnerabilities", []),
            description=data.get("description", ""),
            expected_severity=data.get("expected_severity", 5.0)
        )

@dataclass
class BenchmarkSuite:
    """
    Data class representing a suite of benchmark tests.
    
    Attributes:
        name: Name of the benchmark suite
        description: Description of the benchmark suite
        test_cases: List of test cases in the suite
        version: Version of the benchmark suite
        created_at: When the suite was created
        updated_at: When the suite was last updated
    """
    name: str
    description: str
    test_cases: List[VulnerabilityTestCase]
    version: str = "1.0.0"
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "name": self.name,
            "description": self.description,
            "test_cases": [tc.to_dict() for tc in self.test_cases],
            "version": self.version,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'BenchmarkSuite':
        """Create from dictionary after deserialization."""
        test_cases = [
            VulnerabilityTestCase.from_dict(tc_data)
            for tc_data in data.get("test_cases", [])
        ]
        
        return cls(
            name=data.get("name", ""),
            description=data.get("description", ""),
            test_cases=test_cases,
            version=data.get("version", "1.0.0"),
            created_at=datetime.fromisoformat(data.get("created_at", datetime.now().isoformat())),
            updated_at=datetime.fromisoformat(data.get("updated_at", datetime.now().isoformat()))
        )
    
    def get_test_cases_by_type(self, vulnerability_type: str) -> List[VulnerabilityTestCase]:
        """Get all test cases for a specific vulnerability type."""
        return [tc for tc in self.test_cases if tc.vulnerability_type == vulnerability_type]
    
    def get_vulnerability_types(self) -> List[str]:
        """Get a list of all vulnerability types in the suite."""
        return list(set(tc.vulnerability_type for tc in self.test_cases))


class VulnerabilityBenchmark:
    """
    Class for benchmarking miner performance on different vulnerability types.
    
    This class runs standardized tests against miners and generates comparative
    reports on their performance for different vulnerability types.
    """
    
    def __init__(self, 
                 storage_path: Optional[str] = None,
                 similarity_threshold: float = 0.7,
                 location_tolerance: int = 2):
        """
        Initialize the benchmark system.
        
        Args:
            storage_path: Path to store benchmark data
            similarity_threshold: Threshold for considering a finding to match a labeled vulnerability
            location_tolerance: Number of lines of tolerance for vulnerability location
        """
        # Configuration
        self.similarity_threshold = similarity_threshold
        self.location_tolerance = location_tolerance
        
        # Storage setup
        if storage_path:
            self.storage_path = Path(storage_path)
        else:
            self.storage_path = Path(os.path.expanduser("~/.bitsec/benchmarks"))
        
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # Data structures
        self.benchmark_suites: Dict[str, BenchmarkSuite] = {}
        self.benchmark_results: Dict[str, List[BenchmarkResult]] = defaultdict(list)
        
        # Load existing benchmark suites and results
        self._load_benchmark_data()
        
        # Set up logging
        self.logger = logging.getLogger("bitsec.vulnerability_benchmark")
    
    def load_benchmark_suite(self, suite_path: str) -> bool:
        """
        Load a benchmark suite from a file.
        
        Args:
            suite_path: Path to the benchmark suite JSON file
            
        Returns:
            True if the suite was loaded successfully
        """
        try:
            with open(suite_path, 'r') as f:
                data = json.load(f)
            
            suite = BenchmarkSuite.from_dict(data)
            self.benchmark_suites[suite.name] = suite
            self.logger.info(f"Loaded benchmark suite: {suite.name} with {len(suite.test_cases)} test cases")
            return True
            
        except Exception as e:
            self.logger.error(f"Error loading benchmark suite: {e}")
            self.logger.error(traceback.format_exc())
            return False
    
    def create_default_benchmark_suite(self) -> BenchmarkSuite:
        """
        Create a default benchmark suite with standard test cases.
        
        Returns:
            The created benchmark suite
        """
        test_cases = []
        
        # Create a reentrancy test case
        reentrancy_test = VulnerabilityTestCase(
            vulnerability_type="reentrancy",
            name="Simple Reentrancy",
            code="""
            pragma solidity ^0.8.0;

            contract VulnerableContract {
                mapping(address => uint256) public balances;
                
                function deposit() public payable {
                    balances[msg.sender] += msg.value;
                }
                
                function withdraw(uint256 amount) public {
                    require(balances[msg.sender] >= amount, "Insufficient balance");
                    
                    (bool success, ) = msg.sender.call{value: amount}("");
                    require(success, "Transfer failed");
                    
                    balances[msg.sender] -= amount;
                }
            }
            """,
            labeled_vulnerabilities=[
                {
                    "type": "reentrancy",
                    "file_path": "VulnerableContract.sol",
                    "line_start": 14,
                    "line_end": 14,
                    "severity": 9.0,
                    "description": "Vulnerable to reentrancy attack because it sends ETH before updating the balance"
                }
            ],
            description="A simple contract vulnerable to reentrancy because it sends ETH before updating the balance",
            expected_severity=9.0
        )
        test_cases.append(reentrancy_test)
        
        # Create an integer overflow test case
        overflow_test = VulnerabilityTestCase(
            vulnerability_type="overflow",
            name="Integer Overflow",
            code="""
            pragma solidity ^0.6.0;

            contract VulnerableToOverflow {
                mapping(address => uint256) public balances;
                
                function transfer(address to, uint256 amount) public {
                    // Check if the sender has enough
                    require(balances[msg.sender] >= amount, "Insufficient balance");
                    
                    // Subtract from sender
                    balances[msg.sender] -= amount;
                    
                    // Add to recipient (vulnerable to overflow)
                    balances[to] += amount;
                }
            }
            """,
            labeled_vulnerabilities=[
                {
                    "type": "overflow",
                    "file_path": "VulnerableToOverflow.sol",
                    "line_start": 13,
                    "line_end": 13,
                    "severity": 8.0,
                    "description": "Integer overflow vulnerability in balance addition"
                }
            ],
            description="A contract with an integer overflow vulnerability in Solidity < 0.8.0",
            expected_severity=8.0
        )
        test_cases.append(overflow_test)
        
        # Create an access control test case
        access_control_test = VulnerabilityTestCase(
            vulnerability_type="weak-access-control",
            name="Missing Access Control",
            code="""
            pragma solidity ^0.8.0;

            contract VulnerableAccess {
                address public owner;
                uint256 public treasuryBalance;
                
                constructor() {
                    owner = msg.sender;
                }
                
                function addToTreasury() public payable {
                    treasuryBalance += msg.value;
                }
                
                function withdrawTreasury(uint256 amount) public {
                    // No access control here!
                    payable(msg.sender).transfer(amount);
                    treasuryBalance -= amount;
                }
                
                function transferOwnership(address newOwner) public {
                    // Missing access control
                    owner = newOwner;
                }
            }
            """,
            labeled_vulnerabilities=[
                {
                    "type": "weak-access-control",
                    "file_path": "VulnerableAccess.sol",
                    "line_start": 14,
                    "line_end": 18,
                    "severity": 9.0,
                    "description": "Missing access control in treasury withdrawal function"
                },
                {
                    "type": "weak-access-control",
                    "file_path": "VulnerableAccess.sol",
                    "line_start": 21,
                    "line_end": 24,
                    "severity": 10.0,
                    "description": "Missing access control in ownership transfer function"
                }
            ],
            description="A contract with missing access control on critical functions",
            expected_severity=9.5
        )
        test_cases.append(access_control_test)
        
        # Create a benchmark suite
        suite = BenchmarkSuite(
            name="Default Vulnerability Test Suite",
            description="Standard test cases for common smart contract vulnerabilities",
            test_cases=test_cases
        )
        
        # Save the suite
        self.benchmark_suites[suite.name] = suite
        self._save_benchmark_suite(suite)
        
        return suite
    
    def run_benchmark(self, 
                      suite_name: str,
                      miner_endpoints: Dict[str, Any],
                      vulnerability_types: Optional[List[str]] = None) -> Dict[str, List[BenchmarkResult]]:
        """
        Run benchmark tests against a set of miners.
        
        Args:
            suite_name: Name of the benchmark suite to use
            miner_endpoints: Dictionary mapping miner IDs to their endpoints or client objects
            vulnerability_types: Optional list of vulnerability types to test (None = all types)
            
        Returns:
            Dictionary mapping vulnerability types to lists of benchmark results
        """
        if suite_name not in self.benchmark_suites:
            self.logger.error(f"Benchmark suite '{suite_name}' not found")
            return {}
        
        suite = self.benchmark_suites[suite_name]
        self.logger.info(f"Running benchmark suite: {suite_name}")
        
        # Filter test cases by vulnerability type if specified
        test_cases = suite.test_cases
        if vulnerability_types:
            test_cases = [tc for tc in test_cases if tc.vulnerability_type in vulnerability_types]
        
        # Group test cases by vulnerability type
        test_cases_by_type = defaultdict(list)
        for tc in test_cases:
            test_cases_by_type[tc.vulnerability_type].append(tc)
        
        # Store results by vulnerability type
        results = defaultdict(list)
        
        # Run tests for each miner
        for miner_id, miner_endpoint in miner_endpoints.items():
            self.logger.info(f"Benchmarking miner: {miner_id}")
            
            # Test each vulnerability type
            for vuln_type, type_test_cases in test_cases_by_type.items():
                # Run all test cases for this vulnerability type and collect results
                miner_result = self._run_miner_tests(miner_id, miner_endpoint, vuln_type, type_test_cases)
                
                if miner_result:
                    results[vuln_type].append(miner_result)
                    self.benchmark_results[vuln_type].append(miner_result)
        
        # Save benchmark results
        self._save_benchmark_results()
        
        return results
    
    def _run_miner_tests(self, 
                        miner_id: str, 
                        miner_endpoint: Any,
                        vulnerability_type: str,
                        test_cases: List[VulnerabilityTestCase]) -> Optional[BenchmarkResult]:
        """
        Run a set of test cases for a vulnerability type against a miner.
        
        Args:
            miner_id: ID of the miner being tested
            miner_endpoint: Miner endpoint or client object
            vulnerability_type: Type of vulnerability being tested
            test_cases: List of test cases to run
            
        Returns:
            BenchmarkResult for this miner and vulnerability type, or None if failed
        """
        try:
            self.logger.info(f"Testing {miner_id} on {vulnerability_type} ({len(test_cases)} test cases)")
            
            # Track metrics
            true_positives = 0
            false_positives = 0
            false_negatives = 0
            response_times = []
            
            # Run each test case
            for tc in test_cases:
                # Query the miner with the test case code
                start_time = datetime.now()
                miner_response = self._query_miner(miner_endpoint, tc.code)
                end_time = datetime.now()
                response_time = (end_time - start_time).total_seconds()
                response_times.append(response_time)
                
                # Extract vulnerabilities from response
                found_vulnerabilities = self._extract_vulnerabilities(miner_response)
                
                # Compare against labeled vulnerabilities
                for labeled in tc.labeled_vulnerabilities:
                    if self._matches_any_vulnerability(labeled, found_vulnerabilities):
                        true_positives += 1
                    else:
                        false_negatives += 1
                
                # Count false positives (findings that don't match any labeled vulnerability)
                for found in found_vulnerabilities:
                    if not self._matches_any_vulnerability(found, tc.labeled_vulnerabilities):
                        false_positives += 1
            
            # Calculate average response time
            avg_response_time = sum(response_times) / len(response_times) if response_times else 0
            
            # Create benchmark result
            result = BenchmarkResult(
                vulnerability_type=vulnerability_type,
                miner_id=miner_id,
                true_positives=true_positives,
                false_positives=false_positives,
                false_negatives=false_negatives,
                avg_response_time=avg_response_time,
                timestamp=datetime.now()
            )
            
            self.logger.info(f"Benchmark result for {miner_id} on {vulnerability_type}: "
                            f"F1={result.f1_score:.2f}, Precision={result.precision:.2f}, Recall={result.recall:.2f}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error running tests for {miner_id} on {vulnerability_type}: {e}")
            self.logger.error(traceback.format_exc())
            return None
    
    def _query_miner(self, miner_endpoint: Any, code: str) -> Any:
        """
        Query a miner with a test case.
        
        This method should be adapted to use the actual protocol for communicating with miners.
        
        Args:
            miner_endpoint: Miner endpoint or client object
            code: Code to scan for vulnerabilities
            
        Returns:
            Miner response
        """
        # This is a placeholder - replace with actual miner query logic
        # For now, just return a mock response for testing
        self.logger.info(f"Querying miner (mock implementation)")
        
        # This is where you would implement the actual query to the miner
        # For example:
        # if isinstance(miner_endpoint, YourMinerClient):
        #     return miner_endpoint.query(code)
        
        # For testing, return a mock response
        return {
            "vulnerabilities": [
                {
                    "type": "reentrancy",
                    "file_path": "VulnerableContract.sol",
                    "line_number": 14,
                    "severity": 9.0,
                    "description": "Potential reentrancy vulnerability"
                }
            ]
        }
    
    def _extract_vulnerabilities(self, response: Any) -> List[Dict[str, Any]]:
        """
        Extract vulnerabilities from a miner response.
        
        This method should be adapted based on the actual response format.
        
        Args:
            response: Miner response
            
        Returns:
            List of extracted vulnerabilities
        """
        # Extract vulnerabilities based on the response format
        # This is a placeholder - adjust for the actual response format
        
        if isinstance(response, dict) and 'vulnerabilities' in response:
            return response['vulnerabilities']
        elif hasattr(response, 'vulnerabilities'):
            return response.vulnerabilities
        else:
            self.logger.warning(f"Could not extract vulnerabilities from response")
            return []
    
    def _matches_any_vulnerability(self, 
                                  vuln1: Dict[str, Any], 
                                  vuln_list: List[Dict[str, Any]]) -> bool:
        """
        Check if a vulnerability matches any in a list.
        
        Args:
            vuln1: The vulnerability to match
            vuln_list: List of vulnerabilities to match against
            
        Returns:
            True if the vulnerability matches any in the list
        """
        for vuln2 in vuln_list:
            if self._vulnerabilities_match(vuln1, vuln2):
                return True
        return False
    
    def _vulnerabilities_match(self, vuln1: Dict[str, Any], vuln2: Dict[str, Any]) -> bool:
        """
        Check if two vulnerabilities are the same.
        
        Args:
            vuln1: First vulnerability
            vuln2: Second vulnerability
            
        Returns:
            True if the vulnerabilities match
        """
        # Check vulnerability type
        type1 = vuln1.get('type', '')
        type2 = vuln2.get('type', '')
        if type1 != type2:
            return False
        
        # Check file path
        file1 = vuln1.get('file_path', '')
        file2 = vuln2.get('file_path', '')
        if file1 != file2:
            return False
        
        # Check line location with tolerance
        line1 = vuln1.get('line_start', vuln1.get('line_number', 0))
        line2 = vuln2.get('line_start', vuln2.get('line_number', 0))
        
        return abs(line1 - line2) <= self.location_tolerance
    
    def generate_benchmark_report(self, 
                                 vulnerability_type: Optional[str] = None,
                                 miner_ids: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Generate a benchmark report for selected vulnerability types and miners.
        
        Args:
            vulnerability_type: Optional vulnerability type to filter results
            miner_ids: Optional list of miner IDs to include in the report
            
        Returns:
            Dictionary containing the benchmark report data
        """
        # Filter results by vulnerability type if specified
        results = self.benchmark_results
        if vulnerability_type:
            results = {vulnerability_type: results.get(vulnerability_type, [])}
        
        # Filter results by miner ID if specified
        if miner_ids:
            for vuln_type in results:
                results[vuln_type] = [r for r in results[vuln_type] if r.miner_id in miner_ids]
        
        # Calculate summary statistics for each vulnerability type
        summary = {}
        
        for vuln_type, type_results in results.items():
            if not type_results:
                continue
            
            # Calculate average metrics across miners
            avg_precision = np.mean([r.precision for r in type_results])
            avg_recall = np.mean([r.recall for r in type_results])
            avg_f1 = np.mean([r.f1_score for r in type_results])
            avg_response_time = np.mean([r.avg_response_time for r in type_results])
            
            # Find best and worst performers
            best_miner = max(type_results, key=lambda r: r.f1_score)
            worst_miner = min(type_results, key=lambda r: r.f1_score)
            
            # Store summary for this vulnerability type
            summary[vuln_type] = {
                "avg_precision": avg_precision,
                "avg_recall": avg_recall,
                "avg_f1": avg_f1,
                "avg_response_time": avg_response_time,
                "best_miner": {
                    "id": best_miner.miner_id,
                    "f1_score": best_miner.f1_score,
                    "precision": best_miner.precision,
                    "recall": best_miner.recall
                },
                "worst_miner": {
                    "id": worst_miner.miner_id,
                    "f1_score": worst_miner.f1_score,
                    "precision": worst_miner.precision,
                    "recall": worst_miner.recall
                },
                "miner_results": [r.to_dict() for r in type_results]
            }
        
        # Create overall report
        report = {
            "timestamp": datetime.now().isoformat(),
            "vulnerability_types": list(summary.keys()),
            "miner_ids": miner_ids if miner_ids else list(set(
                r.miner_id for results_list in self.benchmark_results.values() 
                for r in results_list
            )),
            "summary": summary
        }
        
        return report
    
    def save_benchmark_report(self, report: Dict[str, Any], output_path: Optional[str] = None) -> str:
        """
        Save a benchmark report to disk.
        
        Args:
            report: Benchmark report to save
            output_path: Optional path to save the report
            
        Returns:
            Path where the report was saved
        """
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = str(self.storage_path / f"benchmark_report_{timestamp}.json")
        
        try:
            with open(output_path, 'w') as f:
                json.dump(report, f, indent=2)
            
            self.logger.info(f"Saved benchmark report to {output_path}")
            return output_path
            
        except Exception as e:
            self.logger.error(f"Error saving benchmark report: {e}")
            return ""
    
    def generate_html_report(self, report: Dict[str, Any]) -> str:
        """
        Generate an HTML version of the benchmark report.
        
        Args:
            report: Benchmark report data
            
        Returns:
            HTML string representation of the report
        """
        try:
            # Generate comparison charts
            f1_chart = self._generate_f1_chart(report)
            time_chart = self._generate_time_chart(report)
            
            # Convert to HTML
            html = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <title>Vulnerability Benchmark Report</title>
                <style>
                    body {{ font-family: Arial, sans-serif; line-height: 1.6; color: #333; max-width: 1200px; margin: 0 auto; padding: 20px; }}
                    h1, h2, h3 {{ color: #2c3e50; }}
                    .header {{ margin-bottom: 20px; }}
                    .chart-container {{ display: flex; justify-content: space-between; margin: 20px 0; }}
                    .chart {{ width: 48%; }}
                    .stats-table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
                    .stats-table th, .stats-table td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                    .stats-table th {{ background-color: #f8f9fa; }}
                    .section {{ margin-bottom: 30px; }}
                    .vuln-section {{ border: 1px solid #ddd; border-radius: 5px; padding: 15px; margin-bottom: 15px; }}
                </style>
            </head>
            <body>
                <div class="header">
                    <h1>Vulnerability Benchmark Report</h1>
                    <p><strong>Generated:</strong> {report.get("timestamp", datetime.now().isoformat())}</p>
                    <p><strong>Vulnerability Types:</strong> {", ".join(report.get("vulnerability_types", []))}</p>
                    <p><strong>Miners Evaluated:</strong> {len(report.get("miner_ids", []))}</p>
                </div>
                
                <div class="chart-container">
                    <div class="chart">
                        <h3>F1 Scores by Vulnerability Type</h3>
                        <img src="data:image/png;base64,{f1_chart}" alt="F1 Scores" style="max-width: 100%;">
                    </div>
                    <div class="chart">
                        <h3>Average Response Times</h3>
                        <img src="data:image/png;base64,{time_chart}" alt="Response Times" style="max-width: 100%;">
                    </div>
                </div>
            """
            
            # Add sections for each vulnerability type
            summary = report.get("summary", {})
            for vuln_type, vuln_data in summary.items():
                html += f"""
                <div class="vuln-section">
                    <h2>{vuln_type}</h2>
                    <div class="stats-summary">
                        <p><strong>Average F1 Score:</strong> {vuln_data.get("avg_f1", 0):.2f}</p>
                        <p><strong>Average Precision:</strong> {vuln_data.get("avg_precision", 0):.2f}</p>
                        <p><strong>Average Recall:</strong> {vuln_data.get("avg_recall", 0):.2f}</p>
                        <p><strong>Average Response Time:</strong> {vuln_data.get("avg_response_time", 0):.2f} seconds</p>
                    </div>
                    
                    <div class="best-worst">
                        <h3>Performance Highlights</h3>
                        <p><strong>Best Performer:</strong> {vuln_data.get("best_miner", {}).get("id", "")} 
                           (F1: {vuln_data.get("best_miner", {}).get("f1_score", 0):.2f})</p>
                        <p><strong>Worst Performer:</strong> {vuln_data.get("worst_miner", {}).get("id", "")} 
                           (F1: {vuln_data.get("worst_miner", {}).get("f1_score", 0):.2f})</p>
                    </div>
                    
                    <h3>Detailed Results</h3>
                    <table class="stats-table">
                        <tr>
                            <th>Miner ID</th>
                            <th>Precision</th>
                            <th>Recall</th>
                            <th>F1 Score</th>
                            <th>True Positives</th>
                            <th>False Positives</th>
                            <th>False Negatives</th>
                            <th>Response Time (s)</th>
                        </tr>
                """
                
                for miner_result in vuln_data.get("miner_results", []):
                    html += f"""
                        <tr>
                            <td>{miner_result.get("miner_id", "")}</td>
                            <td>{miner_result.get("precision", 0):.2f}</td>
                            <td>{miner_result.get("recall", 0):.2f}</td>
                            <td>{miner_result.get("f1_score", 0):.2f}</td>
                            <td>{miner_result.get("true_positives", 0)}</td>
                            <td>{miner_result.get("false_positives", 0)}</td>
                            <td>{miner_result.get("false_negatives", 0)}</td>
                            <td>{miner_result.get("avg_response_time", 0):.2f}</td>
                        </tr>
                    """
                
                html += """
                    </table>
                </div>
                """
            
            html += """
            </body>
            </html>
            """
            
            return html
            
        except Exception as e:
            self.logger.error(f"Error generating HTML report: {e}")
            return f"<html><body><h1>Error generating report</h1><p>{str(e)}</p></body></html>"
    
    def _generate_f1_chart(self, report: Dict[str, Any]) -> str:
        """Generate base64-encoded F1 score comparison chart."""
        try:
            # Clear any existing plots
            plt.figure(figsize=(10, 6))
            
            # Prepare data
            summary = report.get("summary", {})
            vuln_types = list(summary.keys())
            
            # Create DataFrame for plotting
            data = []
            miner_ids = set()
            
            for vuln_type, vuln_data in summary.items():
                for miner_result in vuln_data.get("miner_results", []):
                    miner_id = miner_result.get("miner_id", "")
                    miner_ids.add(miner_id)
                    data.append({
                        "Vulnerability Type": vuln_type,
                        "Miner ID": miner_id,
                        "F1 Score": miner_result.get("f1_score", 0)
                    })
            
            if not data:
                return ""
            
            df = pd.DataFrame(data)
            
            # Group by vulnerability type and miner ID
            pivot = df.pivot_table(
                index="Vulnerability Type", 
                columns="Miner ID", 
                values="F1 Score", 
                aggfunc="mean"
            )
            
            # Plot
            ax = pivot.plot(kind="bar", figsize=(10, 6))
            plt.title("F1 Scores by Vulnerability Type and Miner")
            plt.xlabel("Vulnerability Type")
            plt.ylabel("F1 Score")
            plt.legend(title="Miner ID")
            plt.tight_layout()
            
            # Save to bytes buffer
            buffer = BytesIO()
            plt.savefig(buffer, format='png', bbox_inches='tight')
            plt.close()
            
            # Convert to base64
            buffer.seek(0)
            image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            return image_base64
            
        except Exception as e:
            self.logger.error(f"Error generating F1 chart: {e}")
            return ""
    
    def _generate_time_chart(self, report: Dict[str, Any]) -> str:
        """Generate base64-encoded response time comparison chart."""
        try:
            # Clear any existing plots
            plt.figure(figsize=(10, 6))
            
            # Prepare data
            summary = report.get("summary", {})
            vuln_types = list(summary.keys())
            
            # Create DataFrame for plotting
            data = []
            miner_ids = set()
            
            for vuln_type, vuln_data in summary.items():
                for miner_result in vuln_data.get("miner_results", []):
                    miner_id = miner_result.get("miner_id", "")
                    miner_ids.add(miner_id)
                    data.append({
                        "Vulnerability Type": vuln_type,
                        "Miner ID": miner_id,
                        "Response Time": miner_result.get("avg_response_time", 0)
                    })
            
            if not data:
                return ""
            
            df = pd.DataFrame(data)
            
            # Group by vulnerability type and miner ID
            pivot = df.pivot_table(
                index="Vulnerability Type", 
                columns="Miner ID", 
                values="Response Time", 
                aggfunc="mean"
            )
            
            # Plot
            ax = pivot.plot(kind="bar", figsize=(10, 6))
            plt.title("Average Response Times by Vulnerability Type and Miner")
            plt.xlabel("Vulnerability Type")
            plt.ylabel("Response Time (seconds)")
            plt.legend(title="Miner ID")
            plt.tight_layout()
            
            # Save to bytes buffer
            buffer = BytesIO()
            plt.savefig(buffer, format='png', bbox_inches='tight')
            plt.close()
            
            # Convert to base64
            buffer.seek(0)
            image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            return image_base64
            
        except Exception as e:
            self.logger.error(f"Error generating time chart: {e}")
            return ""
    
    def _save_benchmark_suite(self, suite: BenchmarkSuite) -> None:
        """Save a benchmark suite to disk."""
        try:
            output_path = self.storage_path / f"{suite.name.replace(' ', '_')}.json"
            with open(output_path, 'w') as f:
                json.dump(suite.to_dict(), f, indent=2)
            
            self.logger.info(f"Saved benchmark suite to {output_path}")
            
        except Exception as e:
            self.logger.error(f"Error saving benchmark suite: {e}")
    
    def _save_benchmark_results(self) -> None:
        """Save all benchmark results to disk."""
        try:
            results_dict = {}
            
            for vuln_type, results in self.benchmark_results.items():
                results_dict[vuln_type] = [r.to_dict() for r in results]
            
            output_path = self.storage_path / "benchmark_results.json"
            with open(output_path, 'w') as f:
                json.dump(results_dict, f, indent=2)
            
            self.logger.info(f"Saved benchmark results to {output_path}")
            
        except Exception as e:
            self.logger.error(f"Error saving benchmark results: {e}")
    
    def _load_benchmark_data(self) -> None:
        """Load benchmark suites and results from disk."""
        # Load benchmark suites
        try:
            suite_files = list(self.storage_path.glob("*.json"))
            
            for suite_file in suite_files:
                if suite_file.name != "benchmark_results.json":
                    try:
                        with open(suite_file, 'r') as f:
                            data = json.load(f)
                        
                        suite = BenchmarkSuite.from_dict(data)
                        self.benchmark_suites[suite.name] = suite
                    except Exception as e:
                        self.logger.error(f"Error loading benchmark suite {suite_file}: {e}")
            
            # Load benchmark results
            results_path = self.storage_path / "benchmark_results.json"
            if results_path.exists():
                with open(results_path, 'r') as f:
                    results_dict = json.load(f)
                
                for vuln_type, results in results_dict.items():
                    self.benchmark_results[vuln_type] = [
                        BenchmarkResult.from_dict(r) for r in results
                    ]
            
        except Exception as e:
            self.logger.error(f"Error loading benchmark data: {e}")
